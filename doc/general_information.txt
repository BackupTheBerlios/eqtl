


               General informaion and instructions
               -----------------------------------

                            on

              setting up an Expression QTL project
              ------------------------------------



  This text brings the the reader to the level that the overall concept
  of the presented tools are understood sufficiently well to set up an
  expression QTL project by his/her own.

  This file should be the first to read. References to other files and
  folders are given where appropriate. It should be noted that every
  file of the source code offers description itself, which explains what
  it is doing and what options are expected. Every folder contains the
  file README with further information. This way one is incrementally
  guided through concepts and the source code.



1. Preparations
===============

1.1 Installation: Download of source code
-----------------------------------------

   The software does not come with an installer. This is not required
   since all parts are scripted and directly executable. The sources
   are made available via a git repository on http://eqtl.berlios.de.

   Once adapted to the workings of git, one can in a straight-forward
   manner share efforts between multiple projects. Every site can have
   their very own servers and their very own local changes to the system,
   while those parts that are of interest for other users can be sent
   back to the server.

   A first start is done with

	git clone git://git.berlios.de/eqtl

   to anonymously checkout the source tree. A username to allow
   for uploads back to the server can be added at a later time.


1.2 Database
------------

   The database is the core unit of the whole dataflow.  It is storing the
   results and is used by the website to present the results. Moreover
   it is controlling the computation of the data.  For the description
   of the tables of the database, see the database description file
   which is not there yet but will be soon...

   The name of the database should be that of your project and be
   specified in the configuration files.  The database itself needs to
   contain the following tables.

	   +----------------------+
	   | Tables_in_Database   |
	   +----------------------+
	   | TableWithChipDetails |
	   | computation          |
	   | locus                |
	   | locusInteraction     |
	   | map                  |
	   | qtl                  |
	   | trait                |
	   +----------------------+

   As a start, it is suggested to directly import the schema
   that is put next to this document:

        yourMysqlSettings="-h yourHostname -u yourUsername"
	echo "create database yourDatabasename;"|mysql $yourMysqlSettings
        mysql $yourMysqlSettings yourDatabaseName < DatabaseSchema.txt

   In MySQL call "help alter table" for instructions how to change the
   properties of columns. This should only be required for the table
   representing the chip details and the implementation of enhancements
   to the system.

   TableWithChipDetails:
   ---------------------
   
        This table contains all information about the expression data.
        The name of the table may vary between projects and was thus
        made a parameter for the configuration.

   computation:
   ------------

        This table controls all computations: all jobs that need to
        be computed are one entry in this database. The jobname is in
        the format:

	 [scanone|scantwo]_probesetid_lodThreshold_numberOfPermutations_(covariate1{_add|_int},covariate2{_add|_int}).csv.gz

        Each entry has a unique id, the computation_id which is linked
        to the results of this job.  In that way, old results can be
        determined and can be deleted if a new compuation was done.
        The status of a job can either be

		QUEUED	     it needs to be computed for the first time
		RECALCULATE  job needs to be recalculated
		PROCESSING   following QUEUED when job is executed
			     but not yet uploaded
		REPROCESSING which follows RECALCULATE
		DONE         the results are already uploaded

   locus:
   ------

        This table gives information about the determined loci,
        for example like the chromosome and the centiMorgan
        position.

   locusInteraction:
   -----------------

        This table contains all results from the scantwo analyses.
        It tells which two loci are interacting for a specific
        probeset_id and specific covariates. There is also information
        about the 95 percentile, the LOD score and much more given.
        For a detailed description please see the database documentation.

   map:
   ----

        This table stores the marker information from the 
        experiment.

   qtl:
   ----

        Table representing the results of the scanone analysis.
        Locus, Probeset_ids, LOD-scores, cMorgan positions and
        much more information are given.

   trait:
   ------

        This table contains information about every single probeset
        like expression high, variance, standarddeviaten.


   Emendations of the schema
   -------------------------


   ~ Trait details ~
   
   The name of the table with chip details is free to be set.


   ~ Covariates ~

   Special care should be taken to adjust the names of covariates
   in the table qtl. It is suggested to only use lower case
   names, but there is no technical requirement for that. For
   details on how to perform the change, call "HELP ALTER TABLE"
   in the MySQL shell. Then perform something analogous to

	alter table qtl change
          covariates
          covariates set('sex_add','sex_int',
                         'onset_add','onset_int',
                         'severity_add','severity_int',
                         'aux_add','aux_int');

   The idea behind the above attribute 'covariates' is that
   every covariate can possibly be part of the modelling of
   the QTL that is represented by that table entry. The
   covariate can either be in as an additive covariate or as
   an interactive covariate, which also renders it additive.
   All combinations are technically possible. The implementation
   as a set-attribute allows for all these without the need
   to introduce an additional table as a link between covariates
   and QTL.


1.3 Data Preparation
--------------------

   Every project will generate data in a different format. The ultimate
   challenge is to prepare the data in a way that any particular job 
   receives (as a single file) the right covariates, the complete genotyping
   data, and the right gene's expression values to perform the computation.
   Also important it is to have every data file in the right order, i.e. the
   mice in the genotyping should be ordered in the same way that the mice
   in the expression data is. The joining of the data sources should
   be performed prior to the submission of the job, to help avoiding
   potential problems.

   The data shall be formatted in a way that the function qtl::read.cross
   understands it directly. The expression data then commonly is in the 2nd
   or third column, which is identified by the variable phenocol (parameter
   PHENOCOL in conf*/data.conf). 


   Upload of traits table
   ----------------------

   The traits can easily be uploaded with the sole inspection of the file that
   the presents the expression data:

	+-------------------------+-------------+------+-----+---------+-------+
	| Field                   | Type        | Null | Key | Default | Extra |
	+-------------------------+-------------+------+-----+---------+-------+

   core fields, the trait_id is the primary key of this table and links to the
   expression QTL tables.
	| trait_id                | varchar(20) | NO   | PRI |         |       |
	| name                    | varchar(50) | YES  |     | NULL    |       |

   expression data, expected as a comma-separated list, and the individuals that
   provided the expression data in the same order as in 'vals', also expected as
   a comma-separated list.
	| vals                    | text        | YES  |     | NULL    |       |
	| individuals             | text        | YES  |     | NULL    |       |


   core statistical data.     
	| mean                    | float       | YES  |     | NULL    |       |
	| sd                      | float       | YES  |     | NULL    |       |
	| median                  | float       | YES  |     | NULL    |       |
	| variance                | float       | YES  |     | NULL    |       |

   fields storing data on the correlation between genes. There is positive and
   negative regulation, the one traits most correlated and its correlation coefficient
   (rho), and a list of arbitrary length that present the X most correlating genes
   and their correlation coefficients as comma separated lists.
	| traits_pos_cor          | text        | YES  |     | NULL    |       |
	| traits_pos_cor_rho      | text        | YES  |     | NULL    |       |
	| traits_pos_cor_most     | varchar(20) | YES  |     | NULL    |       |
	| traits_pos_cor_most_rho | float       | YES  |     | NULL    |       |
	| traits_neg_cor          | text        | YES  |     | NULL    |       |
	| traits_neg_cor_rho      | text        | YES  |     | NULL    |       |
	| traits_neg_cor_most     | varchar(20) | YES  |     | NULL    |       |
	| traits_neg_cor_most_rho | float       | YES  |     | NULL    |       |
	+-------------------------+-------------+------+-----+---------+-------+
	 

   None of the data is ultimately essential for the computation of
   expresssion QTL, in the sense that the distribution of the data for
   the computation will in its current implementation fall back to the
   file-respresentation of expression data.  For the display of data,
   however, and the sharing of the effort to display the data between
   projects, the upload of the data to the database is preferable. To
   amend the current scripts for retrieving all their data from the
   tables is work in progress.

   The scripts that perform the subsequent updates of the trait table
   are suggested not to access the database directly. To allow for manual
   verification, it is preferable to let the scripts output SQL statements
   to STDOUT and pipe that to the database client. For instance, the
   initial upload to the traits table could only contain the trait_id
   and the name, which possibly are also the first and second column of
   the possibly tab-separated text file. To have the records with the
   primary keys uploaded, and thus prepared for subsequent updates with
   other data, one shall execute a Perl script in analogy to

        $ cat <<EOPERL > trait_upload.pl
	#!/usr/bin/perl
	use strict;
	while(<>){
		chomp;
		my @fields=split(/\t/,$_);
		print "INSERT INTO trait SET trait_id='$fields[0]',name='$fields[1]';\n";
	}
	EOPERL
	$ perl trait_upload.pl expressiondata.tsv | \
		mysql -h yourhost -u yourself yourdatabase

   Templates for the reading of expression data for deriving the
   autocorrelation matrix and the correlation between expression data
   and classical phenotypes can be found in scripts/analyses. The data
   of the latter goes into the table trait_phen_cor:

	desc trait_phen_cor;
	+----------+-------------+------+-----+---------+-------+
	| Field    | Type        | Null | Key | Default | Extra |
	+----------+-------------+------+-----+---------+-------+

    ID of trait (expression data) inspected 
	| trait_id | varchar(20) | YES  |     | NULL    |       |

    name of classifical phen
	| phen     | varchar(20) | YES  |     | NULL    |       |

    correlaction coefficient
	| rho      | float       | YES  |     | NULL    |       |

    p-value to observe the correlation with uncorrelated variables
	| p        | float       | YES  |     | NULL    |       |
	+----------+-------------+------+-----+---------+-------+

   


   Subsequently, one should prepare the local data in a way that the
   script

   

1.4 Config files and template files
-----------------------------------

   Most scripts that are prepared for one experiment are available
   and applicable for all experiments using this infrastructure, i.e.
   all scripts except for those involved in the upload of wet-lab data.
   For the latter, only conceptional drafts are available that should
   be adapted.

   The configuration files in the folder 'conf_template' shall be
   copied into a folder that is named 'conf_projectname', substitute
   'projectname' with the respective name of your project. You may have
   multiple projects maintained in parallel. Then edit all the files in
   conf_projectname/ to suite your project.

   The script 'update.sh' will perform the substitution of all the
   placeholders. The substitutions will be performed in the alphabetic
   order of the filenames of those files that contain the rules.
   Nevertheless, dependencies between rules should be strictly avoided.
   The substitutions will be performed on all files ending with
   ".template" and a new file will be created, without the ".template"
   suffix, that has all the substitutions performed.



2. Website
==========

   Once that update.sh was executed, your website shall be ready. There
   are two websites to take care of. The scripts in

        website/*

   perform the distribution of data and present an overview on the
   current state of data generation to the project participants. The
   jobs that are executed remotely will query the website and request
   new data to be submitted. The setup expected to be ready involves

	- Apache with FastCGI Perl interface
	- a MySQL database

   For Debian install the packages libfcgi-perl, libapache2-mod-fastcgi,
   php5-mysql, libapache2-mod-php5 and ensure that the website folder
   is accessible by the apache.


2.1  Communication of the project's internals
---------------------------------------------

   The script

	website/index.php.template

		provides the web interface to human users
		and also gives an introduction to the working of
		the infrastructure

	website/showSRC.pl.template

		displays source code of scripts on the website

2.2  Data Computation
---------------------

	website/evaluateQuery.R

		is the main script to start the computation of the
		data.  It is executed on the machine that performs the
		computation and resides in this folder only to be easily
		accessible for its distribution.

		To run this script, make sure that R/qtl is installed.
		Started with this script, several files are used
		that need to be on a web server with apache and fcgi.
		The path to those files and the names of the files,
		which can be changed if required need to be determined
		in the config files.


	website/recalc.pl.template

		is the first file to be called as it determines what job
		should be executed next, i.e. what clinical parameter
		shall be modelled by which gene's expression levels with
		which set of covariates and those additive or interacting.

	        This script queries the database for the next job to compute.
		The jobname is a concatenation, joined by interspersing "_"
	     	characters, of the following pameters in the exact same order:
		   [scanone|scantwo]	single or combined effects
		   probesetid		trait to be modelled
		   lodThreshold		always set to 3.6
		   numberOfPermutations	always set to 1000
		   (covariate1{_add|_int},covariate2{_add|_int})
					list of covariates to be taken into account
		The extension _add or _int tells wether the covariate
		should be considered interactive or additive in the
		model.	If this information is missing, the covariate
		is automatically considered to be interactive.

		The specification the syntax of jobnames can obviously
		be changed, but many parts of the code as it is written
		today do depend on the current formatting.

		When no more jobs are pending to be computed, the script
		returns q("no") which causes evaluateQuery.R to stop
		and to leave R.


	website/getRscript.pl.template

		prepares a script that is executed with the statistics
		suite R (http://www.r-project.org) and the library R/qtl
		(http://www.rqtl.org).	Albeit written in Perl, this
		script returns R code.

		This script uses the path to the data file (expression
		data, genotyping and quantitative information
		(covariates)).	The path needs to be specified in the
		config files.

		The format of the required data will vary across projects.
		There is yet not ultimate decision on how to achive
		project independence with respect to data formats and to
		what degree this is achievable. More details are found
		in the 'upload' section of this document.


	website/prepareRqtlInputData.pl.template

		prepares the input for the getRscript.pl from the genotype
		data, the expression data and the clinical parameters,
		all for a single gene to be analysed.

		The data is prepared for to be accepted by the
		qtl::read.cross() function, which is executed from the
		R script that is generated by getRscript.pl.

   After the execution of all those scripts, the result file is written
   to the specified directory (default: ~/myTmp).  These files need to
   be retrieved from he host that performs the computation to be then
   uploaded to the database.

   Just a short checklist before you run evaluateQuery.R:

   - make sure the database is reachable and the computation
     table contains jobs with status QUEUED or RECALCULATE
   - make sure the jobnames have the right syntax
   - make sure you have the right data files and they are
     in the config files
   - make sure evaluateQuery.R, recalc.pl, getRscript.pl
     and prepareRqtlInputData.pl are reachable and updated
     with the latest config files
   - make sure that the paths to the files in the config 
     files are correct

2.2  Invocation and Results Upload to Database
-------------------------------

   For the submission of compute jobs and the upload of results
   please refer to the document 'DataHandling.txt' in this folder.


2.3  Results Presentation
-------------------------

   All data that is presented on the interactive web site
   was at some stage stored in the database.

        website/eqtl/*.php

		represents the web interface for the presentation 
		of all results.

		qtl.php
			presentation of the quantitative effect
			that single locus has

		trait.php
			details on the gene, whose expression levels
			are attempted to be modelled by the genotype.

   
3.  Analyses
============

   Analyses are technically performed on multiple levels. Generally,
   the results from an R/qtl run (performed with the script generated
   by getRscript.pl) should be uploaded to the database and then
   be analysed from there, with no fallback on the raw result data.
   
   A series of scripts in the folder "scripts/analyses" is then executed
   to extend the data with additional information, i.e. the mean
   expression levels of genes across all individuals and their standard
   deviations, and multiple ways to represent eQTL density information.
   Also, the extra fields on cis/trans eQTL are updated in this manner.
   The file 'scripts/analyses/README' also provides a respective 
   overview:

      determineCorrelations.R

         correlations between genes and between genes and phenotypes

      scatter_all.R

          2D scatter plot of chromosomal locations of eQTL and the trait
          Every phenotype is presented on a separate graph.

      showQtlDensity.R

         QTL density plots

      FIXME: move script to right folder

         cis - trans QTL


   The remaining challenge is now to allow the human intuition to 
   prepare queries that integrate all the collected facts.
