=pod

=head1 Preparing jobs, their submission, and the handling of result files

Ann-Kristin Grimm <grimm@inb.uni-luebeck.de>, Steffen ME<ouml>ller <moeller@inb.uni-luebeck.de>

LE<uuml>beck, 2009

This document summarises on how to submit jobs to the grid.

=head2 1. SET JOBS TO RECALCULATE

The MySQL server hosts a database with all the expression QTL data. In there,
the table 'computation' is holding information on the progress of the
calculations for a particular gene and a particular set of covariates.

Those jobs that should be sent to the grid or to a local cluster, should
have their status field set to 'RECALCULATE'. The job submission script
will find all these entries and submit them.

The status change can be performed with the following mysql statement, which
would change the status of all those scanones that are at the moment marked
as 'PROCESSING'. This could for example be the case if some results could
not be obtained because a cluster deleted these jobs or remains unavailable:
   
	use eQTL_Stockholm;
	UPDATE computation SET status="RECALCULATE" WHERE 
		status="PROCESSING" AND application="SCANONE";
  
To set the focus on a particular set of covariates, you may want to constrain
the set of jobs to run:

	UPDATE computation SET status="RECALCULATE"
	 WHERE status="PROCESSING" AND application="SCANONE"
	   AND jobname like '%(none)%';

For an overview on the status of the jobs, do

	SELECT NOW(),application,status,COUNT(*)
	  FROM computation 
	 GROUP BY application,status;

To show the time next to the data with NOW() is rather helpful to monitor
the progress and be pointed to sudden increases or delays in thruput.
Either may be indicative of some problem.


=head2 2. INITIATE CALCULATIONS

The script 'scripts/evaluateQuery.R' performs the sequential request of jobs
and their execution. In a loop, this R script continously fetches an R
script to execute, like a work unit. That R script is tied to a particular
data set.

=head3 START SINGLE JOBS ON REGULAR COMPUTERS

Here the instructions to run 'scripts/evaluateQuery.R' locally.

=over 4

=item Log in to the machine that shall execute the job

=item Make sure the evaluateQuery.R is available on that machine.

This may be via an NFS mounted directory or a direct copy from the source code repository.

=item Start the script and disappear

nohup nice -19 R CMD BATCH evaluateQuery.R    

=back

The script has a series of attributes that are read in as enviroment variables.
One is the TIMEOUT, specified in hours, that shall stop the computations once
that a certain runtime was reached. The other is JOBNOMAX, that informs on the
maximal number of jobs that shall be computed.

=head3 START JOBS IN QUEUEING SYSTEM

Queueing systems, or batch systems, are a way to distribute computations
across a LAN. One may expect those machines to share a single directory,
e.g. via NFS. But this is not required.

=over 4

=item Make sure the R/qtl package is installed on all machines of the queue

=item Make sure all machine have access to a shared folder where to store results

=item Directly use (or adapt) the script 'scripts/submitToTorqueLocally.sh' for your purpose.

=back

The command to start 300 jobs that each run as much as they can in 6 hours is

	NAME=example NODES=any ./submitToTorqueLocally.sh 6 300   

which will have 'example' as part of their job name.


=head3 USING A COMPUTATIONAL GRID

A computational grid, here the NorduGrid, is a network of batch systems.
This way, a regular user may execute jobs locally or remotely in a
unified manner.

=head4 START JOBS ON THE NORDUGRID

For a general introduction on how to use a computational grid for
your purposes, please follow the documentation on http://www.nordugrid.org .
Firstly, prepare a time-limited certificate with which to introduce
yourself to other grid nodes:

If there is this config file, then use it

	. /etc/profile.d/nordugrid.sh

Create certificate

	grid-proxy-init

which will look like this

	Your identity: /O=GermanGrid/OU=UniLuebeck/CN=Steffen Moeller
	Enter GRID pass phrase for this identity:
	Creating proxy ..................................... Done
	Your proxy is valid until: Sun Mar 22 04:38:04 2009

To start the grid jobs you need the job.xrsl file, for which a
template is located in the root of the git repository.
It references two scripts in the 'download' subdirectory of the
web page. Please ensure, that their versions are to your liking.
Then

	cp job.template.xrsl job.xrsl

perform modifications as required, in particular the time allowed
by the shell script and the time reserved on the grid should concur.
To submit the job, run

	ngsub -d 1 -f job.xrsl -c benedict.grid.aau.dk 

Site administrators will worry less about the number of jobs that 
are submitted than about jobs that take awfully long to
complete. Please take that into account.

Two more clusters with the R/qtl runtime environment are:

	fyrgrid.grid.aau.dk
	kiniini.csc.fi

You can start the job with a script (which I will probably add to git)
but for the moment I would advice to do it manually, as I don't know
what the spare capacities of the clusters are, and you should check
how many jobs are running and how many are queued and see in this
way how many you can start.  I can't give you an exact number for
benedict.... for kiniini its 30.

To learn more about the individual cluster and inspect the jobs
of yours that are running, this information is available on
http://www.nordugrid.org/monitor .

=head4 DOWNLOAD GRID JOBS

When the jobs are done, you can retrieve them all with

	ngget -a

or select those you want to download.
   
I haven't defined a storage element to move the results to (but maybe I
will do it before I leave) so take care that you are fast enough with
starting the downloads before they are deleted by the remote server.
And think about the time this download needs. This can be up to one
day or more if there are a lot of data. The situation has now been 
brought some relief since grid jobs are terminating regularly.

The easiest thing is to move all files into one directory. If the job
FAILED, then the data is in the myTmp folder, otherwise you should
unpack the evaluatedQueries.tar and remove the tar files before uploading.
For all files, please change the
group to bio

	chgrp -R bio .

and give those groups the right to write

	chmod -R g+w .

=head2 3. UPLOAD THE DATA TO THE DATABASE:

FIRSTLY, CHECK IF YOUR GIT REPOSITORY IS UP TO DATE!

=over 4

=item  Log in to a machine that has access to the collection of results.

This is needed for the sorting, the latest, but just do it now.

	ssh youruploadnode # might not be required for your setup

=item Change the working directory to the root of the project directory

(the directory that you checked out from the git source code repository)

	cd eqtlGitSourceTree

=item Start the script to upload the files, resultDir is the directory where the results are stored:

	./scripts/iterateThroughFiles_new.pl resultDir

=back

Files that were already uploaded are moved to a fallback directory
for subsequent manual inspection. For the time writing, this is hard 
coded as my home directory.

As I am not sure if you have the rights to write there, you might
want to change the hard-coded paths to the brokenfiles, donefiles
and so on in the iterateThroughFiles_new.pl script. I know, I
should have used a template file. And I will, promised, but
probably not today.

This may take some days, but once it is done, sort the file to the
local disc on youruploadnode to keep all the data:

	./scripts/sort.pl recentResultDirectory longTermResultDirectory

=head2 4. CHECK IF ALL JOBS ARE UPLOADED

When everything is uploaded, have a look at the status of the database:

	SELECT status, application, count(*)
	FROM computation GROUP BY application, status;

This information is also shown on the project's home page.

Don't be irritated by the 18 SCANTWOs that are marked as processing. Those
are the probeset which has no data and therefore the calculations fail. We
should possibly remove those one day or improve the script or R/qtl to
spot this issue.

If there are SCANONES left with status processing and you are sure there
is no data left to upload, you need to recalculate those again and start
from the beginning.
