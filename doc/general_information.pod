=pod

=head1 General information and instructions on setting up an Expression QTL project

This text brings the reader to the level that the overall concept
of the presented tools are understood sufficiently well to set up an
expression QTL project by his/her own.

This file should be the first to read. References to other files and
folders are given where appropriate. It should be noted that every
file of the source code offers description itself, which explains what
it is doing and what options are expected. Every folder contains the
file README with further information. This way one is incrementally
guided through concepts and the source code.



=head2 1. Preparations

=head3 1.1 Installation: Download of source code

The software does not come with an installer. This is not required
since all parts are scripted and directly executable. The sources
are made available via a git repository on http://eqtl.berlios.de.

Once adapted to the workings of git, one can in a straight-forward
manner share efforts between multiple projects. Every site can have
their very own servers and their very own local changes to the system,
while those parts that are also of interest for users of other projects
can be sent back to the server. Conversely, the additions performed
for other projects can be merged with the local setup at any time.

A first start is done with

	git clone git://git.berlios.de/eqtl

to anonymously checkout the source tree. A username to allow
for uploads back to the server can be added at a later time, without
the need to download again.

=head3 1.2 Installation of web server on Debian Linux

The data to be represented is stored on a database. Today, with
ubiquitous access to the Internet, public databases are no longer 
needed to be installed locally. While this system still supports
such local setups, the default should be the use of the public
resources. This renders this system installable on every regular
laptop machine.

The here described setup is prepared for the Debian Linux distribution.
It is very strong in its coverage of biological software. But the
installation can analogously be performed on any operating system.

=head4 1.2.1 Local system for browsing only

This setup is a subset of the full szenario. Here we assume
the local expression QTL database to be available already and
we only want to perpare or own analyses or perform changes
to the presentation.

This szenario is fairly common for developers to improve the
analyses once the major computations have been performed and
the database was filled with expression QTL.

=over 4

The following commands need to be executed as the superuser "root"
or be prepended with the command "sudo", which itself might need
to be installed, first.

=item Install web server:

  apt-get install apache2

=item Add web server's PHP scripting capability:

  apt-get install libapache-mod-php5

=item Allow PHP to access MySQL database:

  apt-get install php5-mysql

=item Allow direct database access from command line:

  apt-get install mysql-client

=back

A web browser pointing to http://localhost should then show a page stating
"It works!".  The git repository is expected to have been placed in a
user's regular directory, we are presuming the path ~/gitpath.  This path
needs to be available via the web server. As we find the easiest way to
achieve such is to link to that directory from the ~/public_html folder:

  [ -d ~/public_html ] || mkdir ~/public_html
  cd ~/public_html
  ln -s ~/gitpath/website projectname

That URL will become available as http://localhost/~user/projectname . To
prevent illegitimate access, deny it for everyone except for those with access
to your machine as a start:

  echo "deny from all" > ~/public_html/projectname/.htaccess
  echo "allow from localhost" >> ~/public_html/projectname/.htaccess

When inspecting it from the web browser, this directory will not yet be available.
The web server needs to be instructed first to serve these:

  cd /etc/apache2/mods-enabled
  sudo ln -s ln -s ../mods-available/userdir.* .
  #sudo vim userdir.conf # change if user directories are not in /home
  sudo /etc/init.d/apache restart # not reload, to find PHP

Check if you have permissions for http://localhost/~user/projectname/ .
After continuing the instructions below, in particular a run to
update -np -q, the websites are then ready.

  

=head4 1.2.2 For having the data locally, too

Do all the steps described in 1.2.1.

=over 4

=item install database

  apt-get install mysql-server

=back

We have not tested it ourselves, but the adaptation for other database systems,
Postgres comes to mind, should be straight forward.

=head4 1.2.3 For doing everything from scratch

Do all the steps described in 1.2.1 and 1.2.2.

=over 4

=item Perl and bash are scripting languages for file uploads

  apt-get install perl bash

=item R for analyses with R/qtl

  apt-get install r-base-core r-cran-qtl 

=back


=head3 1.3 Database

The database is the core unit of the whole dataflow.  It is storing the
results and is used by the website to present the results. Moreover
it is controlling the computation of the data.  For the description
of the tables of the database, see the database schema file in
'doc/DatabaseSchema.sql' which is distributed with the source code.

The name of the database should be that of your project and be
specified in the configuration files.  The database itself needs to
contain the following tables.

	   +----------------------+
	   | Tables_in_Database   |
	   +----------------------+
	   | TableWithChipDetails |
	   | computation          |
	   | locus                |
	   | locusInteraction     |
	   | map                  |
	   | qtl                  |
	   | trait                |
	   +----------------------+

As a start, it is suggested to directly import the schema
that is put next to this document:

        yourMysqlSettings="-h yourHostname -u yourUsername"
	echo "create database yourDatabasename;"|mysql $yourMysqlSettings
        mysql $yourMysqlSettings yourDatabaseName < DatabaseSchema.sql

In MySQL call "help alter table" for instructions how to change the
properties of columns. This should only be required for the table
representing the chip details and the implementation of enhancements
to the system - which we all kindly ask to be communicated back to us.

=over 4

=item TableWithChipDetails
   
This table contains all information about the expression data.
The name of the table may vary between projects and was thus
made a parameter for the configuration.

=item computation

This table controls all computations: all jobs that need to
be computed are one entry in this database. The jobname is in
the format:

	 [scanone|scantwo]_probesetid_lodThreshold_numberOfPermutations_(covariate1{_add|_int},covariate2{_add|_int}).csv.gz

Each entry has a unique id, the computation_id which is linked
to the results of this job.  In that way, old results can be
determined and can be deleted if a new compuation was done.
The status of a job can either be

=over 8

=item QUEUED

it needs to be computed for the first time

=item RECALCULATE

job needs to be recalculated

=item PROCESSING

following QUEUED when job is executed but not yet uploaded

=item REPROCESSING

which follows RECALCULATE

=item DONE

the results are already uploaded

=back

=item locus

This table gives information about the determined loci,
for example like the chromosome and the centiMorgan
position.

=item locusInteraction

This table contains all results from the scantwo analyses.
It tells which two loci are interacting for a specific
probeset_id and specific covariates. There is also information
about the 95 percentile, the LOD score and much more given.
For a detailed description please see the database documentation.

=item map

This table stores the marker information from the 
experiment.

=item qtl

Table representing the results of the scanone analysis.
Locus, Probeset_ids, LOD-scores, cMorgan positions and
much more information are given.

=item trait

This table contains information about every single probeset
like expression height, variance, standard deviation.

=back

=head4 Derving a project from an earlier experiment

We are always checking out the source code of the whole
project again for every little subproject. You may
want to use the the functionality of git to allow branches
of the code. But be aware that such a branch needs to be
available to the web server at any time. 

After creation of a new mysql database for the new project,
the schema needs to be loaded in. This can be done from the
project's source code or, if the schema was recently changed
by you, by dumping it from the existing installation:

  ./database_dump.sh --no-data > your_schema.sql

Be careful not to upload that code to an existing expression QTL
database. We have database users for every single project for
this extra security:

  mysql -h dbhost -u newuser dbnew < your_schema.sql

If the same chip system was used, then copy the data from one
system to another:

  insert into BEARatChip select * from old.BEARatChip;

Your new user needs to have profited from an earlier

  GRANT select ON BEARatChip TO newuser

since otherwise he would not have read access.

=head3 Emendations of the schema

=head4 Trait details
   
The name of the table with chip details is free to be set.


=head4 Covariates

Special care should be taken to adjust the names of covariates
in the table qtl. It is suggested to only use lower case
names, but there is no technical requirement for that. For
details on how to perform the change, call "HELP ALTER TABLE"
in the MySQL shell. Then perform something analogous to

  alter table qtl change covariates
    covariates set('sex_add','sex_int',
                   'onset_add','onset_int',
                   'severity_add','severity_int',
                   'aux_add','aux_int');

The idea behind the above attribute 'covariates' is that
every covariate can possibly be part of the modelling of
the QTL that is represented by that table entry. The
covariate can either be in as an additive covariate or as
an interactive covariate, which also renders it additive.
All combinations are technically possible. The implementation
as a set-attribute allows for all these without the need
to introduce an additional table as a link between covariates
and QTL.


=head3 1.4 Data Preparation

Every project will generate data in a different format. The ultimate
challenge is to prepare the data in a way that any particular job 
receives (as a single file) the right covariates, the complete genotyping
data, and the right gene's expression values to perform the computation.
Also important it is to have every data file in the right order, i.e. the
mice in the genotyping should be ordered in the same way as the mice
in the expression data. The joining of the data sources should
be performed prior to the submission of the job, to help avoiding
potential problems.

The data shall be formatted in a way that the function qtl::read.cross
understands it directly. The expression data then commonly is in the 2nd
or 3rd column, which is identified by the variable phenocol (parameter
PHENOCOL in conf*/data.conf). 


=head4 Upload of traits table

The traits can easily be uploaded with the sole inspection of the file that
presents the expression data:

	+-------------------------+-------------+------+-----+---------+-------+
	| Field                   | Type        | Null | Key | Default | Extra |
	+-------------------------+-------------+------+-----+---------+-------+

core fields, the trait_id is the primary key of this table and links to the
expression QTL tables.
	| trait_id                | varchar(20) | NO   | PRI |         |       |
	| name                    | varchar(50) | YES  |     | NULL    |       |

expression data, expected as a comma-separated list, and the individuals that
provided the expression data in the same order as in 'vals', also expected as
a comma-separated list.
	| vals                    | text        | YES  |     | NULL    |       |
	| individuals             | text        | YES  |     | NULL    |       |

core statistical data.     
	| mean                    | float       | YES  |     | NULL    |       |
	| sd                      | float       | YES  |     | NULL    |       |
	| median                  | float       | YES  |     | NULL    |       |
	| variance                | float       | YES  |     | NULL    |       |

fields storing data on the correlation between genes. There is positive and
negative regulation, the one traits most correlated and its correlation coefficient
(rho), and a list of arbitrary length that present the X most correlating genes
and their correlation coefficients as comma separated lists.
	| traits_pos_cor          | text        | YES  |     | NULL    |       |
	| traits_pos_cor_rho      | text        | YES  |     | NULL    |       |
	| traits_pos_cor_most     | varchar(20) | YES  |     | NULL    |       |
	| traits_pos_cor_most_rho | float       | YES  |     | NULL    |       |
	| traits_neg_cor          | text        | YES  |     | NULL    |       |
	| traits_neg_cor_rho      | text        | YES  |     | NULL    |       |
	| traits_neg_cor_most     | varchar(20) | YES  |     | NULL    |       |
	| traits_neg_cor_most_rho | float       | YES  |     | NULL    |       |
	+-------------------------+-------------+------+-----+---------+-------+
	 

None of the data is ultimately essential for the computation of
expresssion QTL, in the sense that the distribution of the data for
the computation will in its current implementation fall back to the
file-respresentation of expression data.  For the display of data,
however, and the sharing of the effort to display the data between
projects, the upload of the data to the database is preferable. To
amend the current scripts for retrieving all their data from the
tables is work in progress.

The scripts that perform the subsequent updates of the trait table
are suggested not to access the database directly. To allow for manual
verification, it is preferable to let the scripts output SQL statements
to STDOUT and pipe that to the database client. For instance, the
initial upload to the traits table could only contain the trait_id
and the name, which possibly are also the first and second column of
the possibly tab-separated text file. To have the records with the
primary keys uploaded, and thus prepared for subsequent updates with
other data, one shall execute a Perl script in analogy to

        $ cat <<EOPERL > trait_upload.pl
	#!/usr/bin/perl
	use strict;
	while(<>){
		chomp;
		my @fields=split(/\t/,$_);
		print "INSERT INTO trait "
		    . "SET trait_id='$fields[0]',name='$fields[1]';\n";
	}
	EOPERL
	$ perl trait_upload.pl expressiondata.tsv | \
		mysql -h yourhost -u yourself yourdatabase

Templates for the reading of expression data for deriving the
autocorrelation matrix and the correlation between expression data
and classical phenotypes can be found in scripts/analyses. The data
of the latter goes into the table trait_phen_cor:

	desc trait_phen_cor;
	+----------+-------------+------+-----+---------+-------+
	| Field    | Type        | Null | Key | Default | Extra |
	+----------+-------------+------+-----+---------+-------+

ID of trait (expression data) inspected 
	| trait_id | varchar(20) | YES  |     | NULL    |       |

name of classifical phen
	| phen     | varchar(20) | YES  |     | NULL    |       |

correlation coefficient
	| rho      | float       | YES  |     | NULL    |       |

p-value to observe the correlation with uncorrelated variables
	| p        | float       | YES  |     | NULL    |       |
	+----------+-------------+------+-----+---------+-------+

   
Subsequently, one should prepare the local data in a way that the
script can perform the upload.


=head4 Initialisation and updates of computation table

The computation table defines the jobs that shall be distributed. Even
though we have prepared scripts to adjust those tables in an
automated fashion, the manual interaction via the mysql shell is
still a straight-forward manner that we encourage. E.g. to reset those
files that have pending results, still, even though all the uploads
of returned files have already been performed, reschedule them by

  update computation set status="RECALCULATE"
         where application="SCANONE"
	   and status="REPROCESSING" or status = "UNKNOWN";

The status "UNKNOWN" may be used to temporarily remove jobs from
computations, e.g.  possibly because of a programming error affecting
only a known subset of computations.  The state 'QUEUED' is equivalent
to 'RECALCULATE' except for the higher priority that is given to jobs
in QUEUED.

The initial filling of the computation table is best performed with the
script in the folder scripts/db_management. Check with the script's man
pages for details.  To upload new calculations just mention the kind
of application to run as the first argument, then list the covariates:

  $ scripts/db_management/uploadExpectedFiles.pl scanone severity_add,sex_int
  RETRIEVING TRAIT NAMES                  [DONE]
  CREATING EXPECTED FILE INDEX            [DONE]

To get an overview on the files that have been created, try

  $ scripts/db_management/uploadExpectedFiles.pl -ls
  ...
  SCANONE:severity_add,sex_int (1031)
  ...
The parentheses list the number of covariates that are available for
that application and list of covariates.

Some traits may have problems, possibly because these were artificial
and only served the inter-chip normalisation. One then needs to
distinguish between systematic problems to retrieve a result and
such that are caused by a temporal failure of some compute node.
The following query compares the number of compute jobs that are in
'REPROCESSING' pre trait. The traits with systematic problems will have
as many computations in 'REPROCESSING' as there were covariates run:

   select trait_id,count(*) as c from computation
                                 where status = 'REPROCESSING'
                                 group by trait_id;                               
	+----------+---+
	| trait_id | c |
	+----------+---+
	|       10 | 1 |
	|       66 | 9 |
	|      101 | 1 |
	|      102 | 1 |
	|      103 | 1 |
	|      233 | 9 |
	|      273 | 9 |

To select those traits which can still be helped, add 'having c < 9'
to the above query.


=head4 Initialisation of map table

The map is best filled after the first computations have been
performed. It may then be derived from the locus table as follows:

  INSERT INTO map (marker,chr,cmorgan_rqtl)
         SELECT Name,Chr,cMorgan FROM locus
         WHERE Name like "D%" ORDER BY chr,cMorgan;

Inspect the data and the correspondance with entries in the Ensembl
data via the marker.php page.


=head3 1.5 Config files and template files

Most scripts that are prepared for one experiment are available
and applicable for all experiments using this infrastructure, i.e.
all scripts except for those involved in the upload of wet-lab data.
For the latter, only conceptional drafts are available that should
be adapted.

The configuration files in the folder 'conf_template' shall be
copied into a folder that is named 'conf_projectname', substitute
'projectname' with the respective name of your project. You may have
multiple projects maintained in parallel. Then edit all the files in
conf_projectname/ to suite your project.

The script 'update.sh' will perform the substitution of all the
placeholders. The substitutions will be performed in the alphabetic
order of the filenames of those files that contain the rules.
Nevertheless, dependencies between rules should be strictly avoided.
The substitutions will be performed on all files ending with
".template" and a new file will be created, without the ".template"
suffix, that has all the substitutions performed.


=head2 2. Website

Once that update.sh was executed, your website should be ready. There
are two websites to take care of:

=over 4

=item website/*      

for the preparation of the data

=item website/eqtl/*

for the presentation of the results

=back

Those pages should not require to be changed between projects. If 
they are, this change should either be a config parameter 
or be of interest for all projects.

The scripts in website/*
perform the distribution of data and present an overview on the
current state of data generation to the project participants. The
jobs that are executed remotely will query the website and request
new data to be submitted. The setup expected to be ready involves

=over 4

=item Apache with FastCGI Perl interface

=item a MySQL database

=back

For Debian install the packages libfcgi-perl, libapache2-mod-fastcgi,
php5-mysql, libapache2-mod-php5 and ensure that the website folder
is accessible by the apache.


=head3 2.1  Communication of the project's internals

The script

=over 4

=item website/index.php.template

provides the web interface to human users
and also gives an introduction to the working of
the infrastructure

=item website/showSRC.pl.template

displays source code of scripts on the website

=back

=head3 2.2  Data Computation

=over 4

=item website/evaluateQuery.R

is the main script to start the computation of the
data.  It is executed on the machine that performs the
computation and resides in this folder only to be easily
accessible for its distribution.

To run this script, make sure that R/qtl is installed.
Started with this script, several files are used
that need to be on a web server with apache and fcgi.
The path to those files and the names of the files,
which can be changed if required need to be determined
in the config files.


=item website/recalc.pl.template

is the first file to be called as it determines what job
should be executed next, i.e. what clinical parameter
shall be modelled by which gene's expression levels with
which set of covariates and those additive or interacting.

This script queries the database for the next job
to compute.  The jobname is a concatenation, joined by
interspersing "_" characters, of the following pameters
in the exact same order:

=over 8

=item [scanone|scantwo]

single or combined effects

=item probesetid

trait to be modelled

=item lodThreshold

always set to 3.6

=item numberOfPermutations

always set to 1000

=item (covariate1{_add|_int},covariate2{_add|_int})

list of covariates to be taken into account

=back

The extension _add or _int tells wether the covariate
should be considered interactive or additive in the
model.	If this information is missing, the covariate
is automatically considered to be interactive.

The specification the syntax of jobnames can obviously
be changed, but many parts of the code as it is written
today do depend on the current formatting.

When no more jobs are pending to be computed, the script
returns q("no") which causes evaluateQuery.R to stop
and to leave R.


=item website/getRscript.pl.template

prepares a script that is executed with the statistics
suite R (http://www.r-project.org) and the library R/qtl
(http://www.rqtl.org).	Albeit written in Perl, this
script returns R code.

This script uses the path to the data file (expression
data, genotyping and quantitative information
(covariates)).	The path needs to be specified in the
config files.

The format of the required data will vary across projects.
There is yet not ultimate decision on how to achive
project independence with respect to data formats and to
what degree this is achievable. More details are found
in the 'upload' section of this document.

=item website/prepareRqtlInputData.pl.template

prepares the input for the getRscript.pl from the genotype
data, the expression data and the clinical parameters,
all for a single gene to be analysed.

The data is prepared for to be accepted by the
qtl::read.cross() function, which is executed from the
R script that is generated by getRscript.pl.

=back

After the execution of all those scripts, the result file is written
to the specified directory (default: ~/myTmp).  These files need to
be retrieved from he host that performs the computation to be then
uploaded to the database.

Just a short checklist before you run evaluateQuery.R:

=over 4

=item  make sure the database is reachable and the computation table contains jobs with status QUEUED or RECALCULATE

=item make sure the jobnames have the right syntax

=item make sure you have the right data files and they are in the config files

=item make sure evaluateQuery.R, recalc.pl, getRscript.pl and prepareRqtlInputData.pl are reachable and updated with the latest config files

=item make sure that the paths to the files in the config files are correct

=back

=head3 2.2  Invocation and Results Upload to Database

For the submission of compute jobs and the upload of results
please refer to the document 'data_handling.pod' in this folder.


=head3 2.3  Results Presentation

All data that is presented on the interactive web site
was at some stage stored in the database. The files in
"website/eqtl" prepare the web interface for the presentation 
of all results.

The source code
is organised such that the folder "eqtl" appears as a 
subfolder of those websites that help organising the
computations. But this is in no way a requirement.
In the contrary, a major design feature was that the
website could be presented independently.


=over 4

=item qtl.php

presentation of the quantitative effect
that single locus has

=item locus.php

presentation of genetic loci that are statistically associated with the disease

=item locusInteraction.php

filtering of interacting loci

=item trait.php

details on the gene, whose expression levels
are attempted to be modelled by the genotype.

=back

   
=head2 3. Analyses

Analyses are technically performed on multiple levels. Generally,
the results from an R/qtl run (performed with the script generated
by getRscript.pl) should be uploaded to the database and then be
analysed from there, with no fallback on the raw result data.

A series of scripts in the folder "scripts/analyses" is then executed
to extend the data with additional information, i.e. the mean
expression levels of genes across all individuals and their standard
deviations, and multiple ways to represent eQTL density information.
Also, the extra fields on cis/trans eQTL are updated in this manner.
The file 'scripts/analyses/README' also provides a respective overview:

=over 4

=item determineCorrelations.R

correlations between genes and between genes and phenotypes

=item scatter_all.R

2D scatter plot of chromosomal locations of eQTL and the trait
Every phenotype is presented on a separate graph.

=item showQtlDensity.R

QTL density plots

=item FIXME: move script to right folder

cis - trans QTL

=back


The remaining challenge is now to allow the human intuition to prepare
queries that integrate all the collected facts.
