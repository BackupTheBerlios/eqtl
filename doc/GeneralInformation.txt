1. Database
===================

   The database is the core unit of the whole dataflow.
   It is storing the results and is used by the website
   to present the results. Moreover it is controlling the
   computation of the data.
   For the description of the tables of the database,
   see the database description file which is not there
   yet but will be soon...
   I am not sure if there is an upload script to create
   the database but if not there will be one soon...
   The name of the database is irrelevant, as long as
   you write it to the config files. The database itself
   needs to contain the following tables:

	   +--------------------+
	   | Tables_in_Database |
	   +--------------------+
	   | BEARatChip         |
	   | computation        |
	   | locus              |
	   | locusInteraction   |
	   | map                |
	   | qtl                |
	   | trait              |
	   +--------------------+

   BEARatChip:
   -----------
   
   This table contains all information about the expression data.
   This is the only table where the name should be chosen dependent from
   the data, but we haven't included this as a config parameter so at
   the moment this name needs to be used.

   computation:
   ------------

   This table controls all computations:
   all jobs that need to be computed are one entry in this
   database. The jobname is in the format:

	 [scanone|scantwo]_probesetid_lodThreshold_numberOfPermutations_(covariate1{_add|_int},covariate2{_add|_int}).csv.gz

   Each entry has a unique id, the computation_id
   which is linked to the results of this job.
   In that way, old results can be determined and can be
   deleted if a new compuation was done.
   The status of a job can either be QUEUED, which means
   it needs to be computed for the first time, RECALCULATE
   which says this job needs to be recalculated, PROCESSING
   or REPROCESSING which says the job is beeing computed at
   the moment or was computed and not uploaded yet or in the
   status DONE, which means the results are already uploaded.
   For a more precisely description of this table please 
   apply to the seperated database documentation.

   locus:
   ------

   This table gives information about the determined loci,
   for example like the chromosome and the centiMorgan
   position.

   locusInteraction:
   -----------------

   This table contains all results from the scantwo analyses.
   It tells which two loci are interacting for a specific
   probeset_id and specific covariates. There is also information
   about the 95 percentile, the LOD score and much more given.
   For a detailed description please see the database documentation.

   map:
   ----

   This table stores the marker information from the 
   experiment.

   qtl:
   ----

   Table representing the results of the scanone analysis.
   Locus, Probeset_ids, LOD-scores, cMorgan positions and
   much more information are given.

   trait:
   ------

   This table contains information about every single probeset
   like expression high, variance, standarddeviaten.
   


2. Config files and template files
==================================

   Most scripts that are prepared for one experiment are available
   and applicable for all experiments using this infrastructure, i.e.
   all scripts except for those involved in the upload of wet-lab data.
   For the latter, only conceptional drafts are available that should
   be adapted.

   The configuration files in the folder 'conf_template' shall be
   copied into a folder that is named 'conf_projectname', substitute
   'projectname' with the respective name of your project. You may have
   multiple projects maintained in parallel. Then edit all the
   files in conf_projectname/ to suite your project.

   The script 'update.sh' will perform the substitution of all the
   placeholders.



3. Website
==========

   Once that update.sh was executed, your website shall be ready. There
   are two websites to take care of. The scripts in 

        website/*

   perform the distribution of data and present an overview on the
   current state of data generation to the project participants. The
   jobs that are executed remotely will query the website and request
   new data to be submitted. The setup expected to be ready involves

	- Apache with FastCGI Perl interface
	- a MySQL database

   The script

	website/index.php.template

		provides the web interface to human users

	getRscript.pl.template

		prepares a script that is executed with the
		statistics suite R (http://www.r-project.org).

	prepareRqtlInputData.pl.template

	recalc.pl.template

	showSRC.pl.template

   
        website/eqtl/*.php



4. Data Computation
====================

   The main script to start the computation of the data is

	scripts/evaluateQuery.R

   To run this script, make sure that Rqtl is installed.  Started with
   this script, several files are used that need to be on a web server
   with apache and fcgi.  The path to those files and the names of the
   files, which can be changed if required need to be determined in the
   config files. The first file to be called is

	website/recalc.pl

   This script queries the database for the next job to
   compute. The jobname is in the following syntax:
	[scanone|scantwo]_probesetid_lodThreshold_numberOfPermutations_(covariate1{_add|_int},covariate2{_add|_int}).csv.gz
   The extension _add or _int tells wether the covariate
   should be considered interactive or additive in the model.
   If this information is missing, the covariate is 
   automatically considered to be interactive.
   Of course the jobname syntax can be changed, but in
   this case, the code needs to be adopted to the new syntax.
   If there are no more jobs to do, the script returns q("no")
   which causes evaluateQuery.R to stop and to leave R.
   The jobname is parsed, and

	website/getRscript.pl 

   is called. This script returns the R code to be executed.
   This script uses the path to the data file (expression 
   data, genotyping and quntitative information(covariates)).
   The path needs to be specified in the config files.
   The format of the required data will soon appear in this
   file in a special section. Of course you don't need to
   stick with this format specification, but be aware that
   you need to adopt the code if you change it.
   The generated R-code needs the script

	website/prepareRqtlInputData.pl 

   This script generates an output which contains the needed
   information from the data and can be read via read.cross()
   from the R script generated by getRscript.pl.

   After the execution of all those scripts, the result file
   is written to the specified directory (default: ~/myTmp)
   and the results can be uploaded to the database.

   Just a short checklist before you run evaluateQuery.R:

   - make sure the database is reachable and the computation
     table contains jobs with status QUEUED or RECALCULATE
   - make sure the jobnames have the right syntax
   - make sure you have the right data files and they are
     in the config files.
   - make sure evaluateQuery.R, recalc.pl, getRscript.pl
     and prepareRqtlInputData.pl are reachable and updated
     with the latest config files
   - make sure that the pathes to the files in the config 
     files are right

   
5. Data Upload
====================

@Ann-Kristin: to be completed by Benedikt



